\section{Normed Spaces. Banach Spaces}
\setcounter{subsection}{2}
\subsection{Further Properties of Normed Spaces}
  \paragraph{4.}
    cf. Prob. 13, Sec 1.2
  \begin{proof}
    The continuity of addition and multiplication follows respectively from the
    inequalities
    \[
      \|(x_1+y_1)-(x_2+y_2)\| \le \|x_1-x_2\|+\|y_1-y_2\|
    \]
    and 
    \[
      \|\alpha_1x_1-\alpha_2x_2\| = 
      \|\alpha_1x_1-\alpha_1x_2+\alpha_1x_2-\alpha_2x_2\|
      \le |\alpha_1|\|x_1-x_2\| + |\alpha_1-\alpha_2|\|x_2\|.
    \]
  \end{proof}

  \paragraph{7.}
  \begin{proof}
    Let $Y$ and $y_n$ be defined as in the hint. Then $\|y_n\|=1/n^2$, 
    constituting a convergent number series. However, 
    \[
      \sum_{n=1}^N y_n = (1,1/4, \dots, 1/N^2, 0,\dots),
    \]
    which is divergent as $N\to\infty$.
  \end{proof}

  \paragraph{8.}
  \begin{proof}
    Let $(x_n)$ be a Cauchy sequence in $X$. Hence, for every $n>0$, there 
    exists some $K_n>0$ such that for all $p,q>K_n$, $\|x_p-x_q\|<1/n^2$. 
    Without loss of generality, we may assume that $(K_n)$ is increasing. Since
    the series $\|x_{K_{n+1}}-x_{K_n}\|$ is bounded by $1/n^2$, it converges. 
    By the hypothesis, the series $(x_{K_{n+1}}-x_{K_n})$ also converges. Hence, 
    \[
      x_{K_n} = x_{K_1} + \sum_{i=1}^{n-1}(x_{K_{i+1}}-x_{K_i}) \to x
      \quad\text{as } n\to\infty.
    \]\par
    Now we show that $(x_n)$ converges to $x$. For every $\vep>0$, since $(x_n)$
    is a Cauchy sequence, there exists some $N_1$ such that for all $p,q>N_1$,
    $\|x_p-x_q\|<\vep$. Meanwhile, since $x_{K_n}\to x$, once $K_n$ is large
    enough, $\|x-x_{K_n}\|<\vep$. Let $K_n>N_1$. Then for every $n>K_n$
    \[
      \|x_n-x\| \le \|x_n-x_{K_n}\| + \|x_{K_n}-x\| \le 2\vep.
    \]
    Thus, $X$ is complete.
  \end{proof}

  \paragraph{9.}
  \begin{proof}
    Let $(x_n)$ be an absolutely convergent series in Banach space $X$. Let 
    $s_n=\sum_{i=1}^n x_n$. Now we show that $s_n$ is a Cauchy sequence and 
    therefore convergent. Since $\sum_{i=1}^\infty\|x_i\|<\infty$, for every 
    $\vep>0$, there exists some $N>0$ such that for all $n>N$, $\sum_{i=n}
    ^\infty\|x_i\|<\vep$. Hence, for every $N<p\le q$,
    \[
      \|s_q-s_p\| = \left\|\sum_{i=p+1}^q x_i\right\|
      \le \sum_{i=p+1}^q\|x_i\| < \vep,
    \]
    completing the proof.
  \end{proof}

  \paragraph{10.}
  \begin{proof}
    Let $(e_n)$ be Schauder basis of $X$. Denote the underlying field of $X$ by
    $\mathbb{K}$ and let $\mathbb{W}=\mathbb{Q}$ if $\mathbb{K}=\mathbb{R}$ and
    $\mathbb{W}=\{p+iq:\, p,q\in\mathbb{Q}\}$ if $\mathbb{K}=\mathbb{C}$. Now we
    show that
    \[
      S = \left\{\sum_{i=1}^n\alpha_ie_i:\, 
      \alpha_i\in\mathbb{W}, n=1,2,\dots\right\},
    \]
    a countable subset of $X$, is dense in $X$ to derive the separability. \par
    For every $x\in X$ and $\vep>0$, by the definition of Schauder basis, there 
    exists $\beta_1,\dots,\beta_n\in\mathbb{K}$ such that $\|x-(\beta_1e_1+
    \cdots+\beta_ne_n)\|<\vep$. Let $M=\max_{i}\|e_i\|$. If $M=0$, then there is
    nothing to prove. Otherwise, since $\mathbb{W}$ is dense in $\mathbb{K}$,
    for $i=1,\dots,n$, there exists $\alpha_i\in\mathbb{W}$ with $|\alpha_i-
    \beta_i|<\vep/2^iM$. Hence,
    \begin{align*}
      \left\|x-\sum_{i=1}^n\alpha_ie_i\right\|
      \le& \left\|x-\sum_{i=1}^n\beta_ie_i\right\| +
           \left\|\sum_{i=1}^n(\beta_i-\alpha_i)e_i\right\| \\
      \le& \vep + \sum_{i=1}^n|\alpha_i-\beta_i|\|e_i\|  \\
      \le& 2\vep.
    \end{align*}
    Thus, $S$ is dense in $X$ and therefore $X$ is separable.
  \end{proof}

  \paragraph{14.}
  \begin{proof}
    Clear that $\|\cdot\|_0$ is nonnegative. And $\|\alpha\hat{x}\|_0= \inf_{x
    \in\hat{x}}\|\alpha x\| = |\alpha|\|\hat{x}\|_0$. Meanwhile, $\|\hat{x}+
    \hat{y}\|_0=\inf_{z\in\hat{x}+\hat{y}}\|z\|\le\inf_{z\in\hat{x}}\|z\|+
    \inf_{z\in\hat{y}}\|z\|=\|\hat{x}\|_0+\|\hat{y}\|_0$. Finally, we show that
    $\|\hat{x}\|_0=0$ implies $\hat{x}=Y$ and invoke Prob. 4, Sec 2.2 to 
    complete the proof. Since $\|\hat{x}\|_0=0$, there exists $(x_n)\subset
    \hat{x}$ which converges to $0$. Since $Y$ is closed, $Y$ is complete and so
    is its cosets. Therefore, $0\in\hat{x}$, enforcing $\hat{x}$ to be $Y$. 
  \end{proof}
% end

\subsection{Finite Dimensional Normed Spaces}
  \paragraph{3.}
  \begin{proof}
    The reflexive property clearly holds. If there are positive $a$ and $b$ such 
    that $a\|x\|_0\le\|x\|_1\le b\|x\|_0$ for all $x\in X$, then $\|x\|_1/b\le
    \|x\|_0\le\|x\|/a$. Hence the relation is symmetric. Next we further suppose
    there exists positive $c$ and $d$ such that that $c\|x\|_1\le\|x\|_2\le
    d\|x\|_1$. Then $ac\|x\|_0\le \|x\|_2\le bd\|x\|_0$, giving the transitive
    property. Thus, the axioms of an equivalence relation hold.
  \end{proof}

  \paragraph{4.}
  \begin{proof}
    Suppose the norms $\|\cdot\|$ and $\|\cdot\|_0$ are equivalent. Let $E
    \subset X$ be any open set with respect to $\|\cdot\|$, i.e., for every $x_0
    \in E$, there exists some $\delta>0$ such that $A=\{x\in X:\, \|x-x_0\|<
    \delta\}\subset E$. Since $\|\cdot\|\sim\|\cdot\|_0$, there exists some 
    positive $c$ such that $\|x-x_0\|\le c\|x-x_0\|_0$. Hence, $B=\{x\in X:\,
    \|x-x_0\|<\delta/c \}\subset A\subset E$. Namely, $E$ is also open with 
    respect to $\|\cdot\|_0$. Interchanging the roles of $\|\cdot\|$ and 
    $\|\cdot\|_0$ completes the proof.
  \end{proof}
  
  \paragraph{5.}
  \begin{proof}
    Suppose the norms $\|\cdot\|$ and $\|\cdot\|_0$ are equivalent. Then for 
    every $x\in X$, there exists some $c>0$ such that $\|x\|_0\le c\|x\|$. Let
    $(x_n)$ be a Cauchy sequence with respect to $\|\cdot\|$, i.e., for every 
    $\vep>0$, there exists some $N>0$ such that for all $n,m>N$, $\|x_n-x_m\|<
    \vep/c$. Hence, $\|x_n-x_m\|_0<c\|x_n-x_m\|\le \vep$. Thus, $(x_n)$ is also
    a Cauchy with respect to $\|\cdot\|_0$. Interchanging the roles of 
    $\|\cdot\|$ and $\|\cdot\|_0$ completes the proof.
  \end{proof}
% end

\subsection{Compactness and Finite Dimension}
  \paragraph{5.}
  \begin{proof}
    Clear that every point in $\mathbb{R}^n$ or $\mathbb{C}^n$ has a closed 
    bounded, and therefore compact, neighborhood. Hence, $\mathbb{R}^n$ and 
    $\mathbb{C}^n$ are locally compact.
  \end{proof}

  \paragraph{6.}
  \begin{proof}
    Let $X$ be a compact metric space and $x$ any point in $X$. Let $E$ be a 
    closed neighborhood of $x$. By Prob 10, $E$ is compact. Thus, $X$ is locally
    compact.
  \end{proof}

  \paragraph{7.}
  \begin{proof}
    It suffices to show that $a=\inf_{y\in Y}\|v-y\|$ can actually be obtained.
    Let $\{b_1,\dots,b_n\}$ be a basis of $Y$ and $y_k=y_{k,1}b_1+\cdots+y_{k,n}
    b_n$ a sequence in $Y$ with $\|v-y_k\|\to a$. We may assume without loss of
    generality that $\|v-y_k\|$ is bounded. \par
    Since $Y$ is a proper subset of $Z$, $v,b_1,\dots,b_n$ are linearly
    independent. Therefore, by Lemma 2.4-1, there exists a scalar $c>0$ such
    that for every $k$,
    \[
      \|v-y_{k,1}b_1-\cdots-y_{k,n}b_n\| \ge c(1+|y_{k,1}|+\cdots+|y_{k,_n}|).
    \]
    Hence, the sequence $(y_{k,1},\dots,y_{k,n})$ of $n$-tuples is bounded and 
    therefore has a convergent subsequence. Consequently, $(y_k)$ also has a 
    convergent subsequence. Suppose that it converges to $z\in Z$. Note that
    $\|v-z\|=a$ and as $Y$ is closed, $z\in Y$. Thus, $a$ can be attained in 
    $Y$.
  \end{proof}

  \paragraph{8.}
  \begin{proof}
    Since the unit ball $B$ with respect to $\|\cdot\|_2$ in $\mathbb{R}^n$ and
    $\mathbb{C}^n$ is compact and $\|\cdot\|$ is continuous, by 2.5-7, $x\mapsto
    \|x\|$ can attains its minimum, denoted by $a$, on $B$. Due to the positive 
    definite property of a norm, $a$ is positive. Hence, $0<a\le \|x/\|x\|_2\|$.
    Namely, $a\|x\|_2\le\|x\|$.
  \end{proof}

  \paragraph{9.}
  \begin{proof}
    For every $(x_n)\subset M\subset X$, since $X$ is compact, there exists a 
    subsequence $(x_{n_k})$ of $(x_n)$ which converges to some $y\in X$. Since 
    $M$ is closed, $y\in M$. Hence, $M$ is compact.
  \end{proof}

  \paragraph{10.}
  \begin{proof}
    From 1.3-4 and the definition of closed sets, we conclude that a mapping is
    continuous iff the preimage of a closed set under it is also a closed set.
    Hence, to show that the inverse of $T$ is also continuous, it suffices to
    show that the image of a closed set $A\subset X$ under $T$ is again a closed
    set. Since $X$ is compact and $A$ is closed, $A$ is compact. Since $T$ is 
    continuous, by 2.5-6, $T(A)$ is compact and therefore closed. Hence, $T$ is
    a homeomorphism.
  \end{proof}
% end

\subsection{Bounded and Continuous Linear Operators}
  \paragraph{2.}
  \begin{proof}
    First suppose $T$ to be bounded and let $A$ be any bounded set in $X$. Then
    there exists $K<\infty$ such that for all $x\in A$, $\|x\|<K$. Due to the
    boundedness of $T$, $\|Tx\|\le \|T\|\|x\|<K\|T\|$. Namely, $T(A)$ is also
    bounded.\par
    Now suppose that $T$ maps bounded sets in $X$ into bounded sets in $Y$. 
    Clear that the unit ball $B$ of $X$ is bounded and therefore so is $T(B)$.
    Namely, $\|Tx/\|x\|\|$ is bounded for $x\ne 0$.\footnote{Note that the two
    $\|\cdot\|$ here are different norms.} Hence, $T$ is bounded.
  \end{proof}

  \paragraph{3.}
  \begin{proof}
    For every $x$ with $\|x\|<1$, $\|Tx\|\le \|T\|\|x\|<\|T\|$.
  \end{proof}

  \paragraph{4.}
  \begin{proof}
    Suppose that the linear operator $T$ is continuous at $x_0\in
    \mathscr{D}(T)$. For every $(x_n)\subset\mathscr{D}(T)$ with $\|x_n-x\|\to
    0$, by the continuity of $T$ at $x_0$
    \[
      \|Tx_n-Tx\|=\|T(x_n-x+x_0)-Tx_0\| \to 0.
    \]
    Hence, $T$ is continuous.
  \end{proof}

  \paragraph{7.}
  \begin{proof}
    The inequality implies $\mathscr{N}(T)=0$. Hence, by Theorem 2.6-10, $T\inv$
    exists. For every $y\in Y$, suppose that $y=Tx$. Then
    \[
      \|T\inv y\| = \|x\| \le \frac{1}{b}\|Tx\| = \frac{1}{b}\|y\|.
    \]
    Thus, $T\inv$ is bounded.
  \end{proof}

  \paragraph{12.}
  \begin{proof}
    The compatibility follows immediately from the definition of the supremum.
    Suppose $\|x\|_1=\max_j|\xi_j|$ and $\|y\|_2=\max_j\|\eta_j\|$, then
    \[
      Ax = 
      \begin{bmatrix}
        x_1\alpha_{11} + \cdots + x_n\alpha_{1n} \\
        \vdots \\
        x_1\alpha_{r1} + \cdots + x_n\alpha_{rn}.
      \end{bmatrix}
    \]
    Since for all $j$, $x_j\le \|x_j\|_1$,
    \[
      \frac{\max_j|x_1\alpha_{j1}+\cdots+x_n\alpha_{jn}|}{\|x\|_1}
      =\max_j\left|\frac{x_1}{\|x\|_1}\alpha_{j1}+\cdots
      +\frac{x_n}{\|x\|_1}\alpha_{jn}\right|
      \le \max_{j}\sum_{k=1}^n|\alpha_{jk}|.
    \]
    Hence, 
    \begin{equation}
      \label{eq:2.8.12}
      \|A\|\ge \frac{\|Ax\|_2}{\|x\|_1} \quad\text{for all $x$}.
    \end{equation}
    Suppose that maximum of $\sum_{k=1}^n|\alpha_{jk}|$ is obtained at $j=p$.
    Then choosing $x_k$ to be $\sgn \alpha_{pk}$ shows that the equality in 
    \eqref{eq:2.8.12} can actually be attained. Hence, $\|A\|=\max_{j}
    \sum_{k=1}^n|\alpha_{jk}|$.
  \end{proof}
% end
