\section{Eigenvalues, eigenvectors, and similarity}
\setcounter{subsection}{-1}

\subsection{Introduction}

  \paragraph{1.}
  \begin{proof}
    Let $S=\{x\in\mathbb{R}^n\,:\,x^Tx=1\}$, which is clearly a compact subset
    of $\mathbb{R}^n$. Consider the function $f:x\mapsto x^TAx$. Since,
    \[
      \|f(x+\delta)-f(x)\| = \|(x^TA)\delta + \delta^T(Ax) + \delta^TA\delta\|
      \le K\|\delta\|
    \]
    for every $x\in\mathbb{R}$ and some fixed $K$, $f$ is continuous. Hence,
    by Weierstrass's theorem, $f$ attains its maximum value at some point $x\in 
    S$. Namely, (1.0.3) has a solution $x$. Therefore, there exists some 
    $\lambda\in\mathbb{R}$ such that $2(Ax-\lambda x)=0$, implying that every
    real symmetric matrix has at least one real eigenvalue.
  \end{proof}

  \paragraph{2.}
  \begin{proof}
    Let $S=\{x\in\mathbb{R}^n\,:\,x^Tx=1\}$ and $m$ be the maximum value of $x
    \mapsto x^TAx$ in $S$. Suppose $\lambda$ is an eigenvalue of $A$ and $u\ne0$ 
    is its associated eigenvector, then
    \[
      Au=\lambda u \quad\Rightarrow\quad 
      u^TAu=\lambda\|u\|^2 \quad\Rightarrow\quad
      (u/\|u\|)^T A (u/\|u\|) = \lambda \quad\Rightarrow\quad
      m \ge \lambda.
    \]
    Meanwhile, by the previous discussion, $m$ itself is a eigenvalue of $A$.
    Hence, it is the largest real eigenvalue of $A$.
  \end{proof}

% end

\subsection{The eigenvalue-eigenvector equation}
  \paragraph{1.}
  \begin{proof}
    It follows from
    \[
      (A^{-1}-\lambda^{-1}I)x 
      = (A^{-1}-\lambda^{-1}A^{-1}A)x 
      = \lambda^{-1}A^{-1}(\lambda I-A)x = 0.
    \]
  \end{proof}

  \paragraph{3.}
  \begin{proof}
    Since $A\in M_n(\mathbb{R})$, $u,v\in\mathbb{R}^n$ and $\lambda\in
    \mathbb{R}$, 
    \[
      Ax=\lambda x\quad\Rightarrow\quad
      Au + iAv = \lambda u + i\lambda v
    \]
    implies $Au=\lambda u$ and $Av=\lambda v$. As $x\ne 0$, at least one of $u$
    and $v$ is nonzero and therefore $A$ has a real eigenvector associated with
    $\lambda$. It can happen that only one of $u$ and $v$ is an eigenvector of 
    $A$, because if $x\in\mathbb{R}^n$, which may happen as we discussed above,
    the imaginary part of $x$ is $0$. Finally, if $x$ is a real eigenvector of 
    $A$, then the eigenvalue $\lambda$ it associated with must be real. 
    Otherwise, at least one entry of $\lambda x$ is not real as $x\ne 0$, 
    contradicting with the fact that $Ax$ is real. 
  \end{proof}

  \paragraph{5.}
  \begin{proof}
    Let $p(t)=t^2-t$. Since $A$ is idempotent, $p(A)=A^2-A=0$. Hence, $0$ is the
    only eigenvalue of $p(A)$. By Theorem 1.1.6, the only values the eigenvalues
    of $A$ can be are the zeros of $p$, namely, $0$ and $1$. \par
    Suppose $A$ is nonsingular, then multiplying $A^{-1}$ on the both sides of
    $A^2=A$ yields $A=I$.
  \end{proof}

  \paragraph{7.}
  \begin{proof}
    Suppose $\lambda\in\sigma(A)$ and $x$ is its associated eigenvector, then
    \begin{align*}
       & 0 = (A-\lambda I)x = x^*(A^*-\bar{\lambda}I) = x^*(A-\bar{\lambda}I) \\
      \Rightarrow\quad&
      0 = x^*(A-\bar{\lambda}I)x = x^*Ax - \bar{\lambda}x^*x = 
      (\lambda-\bar{\lambda}) \|x\|^2.
    \end{align*}
    Hence, $\lambda=\bar{\lambda}$, implying all eigenvalues of $A$ are real.
  \end{proof}

  \paragraph{9.}
  \begin{solution}
    Solve the equation $\det(A-\lambda I) = 0$ and we get $\lambda=\pm i$.
  \end{solution}

  \paragraph{11.}
  \begin{proof}
    If $\rank(A-\lambda I)<n-1$, then $\adj(A-\lambda I)=0$ by (0.8.2) and 
    therefore we can always choose $y$ to be the $0$ and the other parts of the 
    proposition clearly hold. Hence, in the following discussion, we assume that
    $\rank(A-\lambda I)=n-1$. \par
    Apply the full-rank factorization and we get $\adj(A-\lambda I)=\alpha xy^*$
    for some nonzero $\alpha\in\mathbb{C}$ and $x,y\in\mathbb{C}^n$. Replacing 
    $x$ with $\alpha x$ and $\alpha$ with $1$ proves the first part.\par
    Suppose $\adj(A-\lambda I)=[\beta_1,\dots,\beta_n]$, then
    \[
      (A-\lambda I)\adj(A-\lambda I) \quad\Rightarrow\quad
      (A-\lambda I)\beta_k = 0 \quad(k=1,2,\dots,n),
    \]
    implying that $\beta_k$ is an eigenvector of $A$ associated with $\lambda$ 
    as long as it is nonzero.
  \end{proof}

  \paragraph{13.}
  \begin{proof}
    If $\rank A < n -1$, then $x$ is always an eigenvector of $\adj A$ 
    associated with $0$ as $\adj A = 0$. Hence, we may assume that $\rank = n - 
    1$. Then $\adj A = (\det A)A^{-1}$. By Exercise 1, $x$ is an eigenvector of 
    $A^{-1}$ and therefore an eigenvector of $\adj A$.
  \end{proof}
  
% end

\subsection{The characteristic polynomial and algebraic multiplicity}
  \paragraph{2.}
  \begin{proof}
    Suppose $A = [a_{ij}]_{m,n} = [\alpha_1,\dots,\alpha_n]^T$ and $B = 
    [b_{ij}]_{n,m} = [\beta_1,\dots,\beta_n]$, then
    \[
      \tr(AB)=\sum_{i=1}^n\alpha_i\beta_i
      =\sum_{i=1}^n\sum_{j=1}^ma_{ij}b_{ji}
      =\sum_{j=1}^m\sum_{i=1}^nb_{ji}a_{ij}
      =\tr(BA).
    \]
    Hence, for nonsingular $S\in M_n$, $\tr(S^{-1}AS)=\tr(S(S^{-1}A)) = \tr(A)$.
    \par For $A\in M_n$, $\det(S^{-1}AS) = \det(S)\det(S^{-1})\det(A)=\det(A)$,
    which means the determinant function on $M_n$ is similarity invariant.
  \end{proof}

  \paragraph{4.}
  \begin{proof}
    It follows immediately from the fact that $\sigma(A)\subset\{0, 1\}$ and 
    $S_k(A)$ is the sum of some $\prod\lambda_{i_j}$. 
  \end{proof}

  \paragraph{6.}
  \begin{proof}
    $\rank(A-\lambda I)=n-1$ implies the matrix $A-\lambda I$ is singular, and
    therefore $\lambda$ is an eigenvalue of $A$. However, it may not have 
    multiplicity $1$. For example\footnote{Thanks to Zhihan Jin, one of my 
    classmates.}, suppose $A=\begin{bsmallmatrix} 1 & -1 \\ 1 & -1 
    \end{bsmallmatrix}$. $\rank A = 1$ but $0$, the only eigenvalue of
    $A$ is of multiplicity $2$.
  \end{proof}

  \paragraph{8.}
  \begin{proof}
    $p_{A+\lambda I}(t)=\det(tI-(A+\lambda I))=\det((t-\lambda)I-A)=p_A(t-
    \lambda)$ and hence the eigenvalues of $A+\lambda I$, the zeros of $p_{A+
    \lambda}(t)$, are $\lambda_1+\lambda,\dots, \lambda_n+\lambda$.
  \end{proof}

  \paragraph{10.}
  \begin{proof}
    Since $p_A(t)$ has $n$ roots and non-real roots of a polynomial come in 
    paris, at least one of the roots is real. Hence, $A$ has at least one real
    eigenvalue.
  \end{proof}

  \paragraph{12.} TODO
    
  \paragraph{14.}
  \begin{proof}
    Suppose $C=\begin{bsmallmatrix} \mu & 0 \\ * & B \end{bsmallmatrix}$. By the
    exercise on p52,
    \[
      p_A(t) = (t-\lambda)p_C(t) = (t-\lambda)p_{C^T}(t) 
      = (t-\lambda)(t-\mu)p_B(t).
    \]
  \end{proof}

  \paragraph{16.}
  \begin{proof}
    $f(t)=\det(A+(tx)y^T = \det A + y^T(\adj A)tx = \det A + t\beta$ where 
    $\beta = y^T(\adj A)x$, a constant independent of $t$. Hence, for $t_1\ne
    t_2$
    \[
      \frac{t_2f(t_1)-t_1f(t_2)}{t_2-t_1} 
      = \frac{t_2(\det A + t_1\beta) - t_1(\det A + t_2\beta)}{t_2-t_1}
      = \det A.
    \]
    For the second part, we can get from calculation that
    \[
      f(-b)=\det(A-b[1,\dots,1]^T[1,\dots,1]) = (d_1-b)\cdots(d_n-b)=q(b)
    \]
    and $f(-c)=q(-c)$. Hence, if $b\ne c$,
    \[
      \det A = \frac{(-c)f(-b)- (-b)f(-c)}{(-c)-(-b)}=\frac{bq(c)-cq(b)}{b-c}.
    \]
    Now suppose $b=c$. Note that $f(t)$ is a linear function of $t$, which is 
    differentiable, implying that 
    \[
      \det A = \lim_{t_2\to t_1}\frac{t_2f(t_1)-t_1f(t_2)}{t_2-t_1} 
      = f\hp(t_1)t_1 - f(t_1).
    \]
    Meanwhile, since $q(t)$ is continuous, $q(t)\to f(-b)$ as $t\to b$. Thus,
    \[
      \det A = \lim_{c\to b}\frac{(-c)f(-b)- (-b)f(-c)}{(-c)-(-b)}
      = q(b) - bq\hp(b).
    \]
    Let 
    \[
      A_* = \lambda I - A = 
      \begin{bmatrix}
        \lambda & -b      & \cdots & -b \\
        -c      & \lambda & \ddots & \vdots \\
        \vdots  & \ddots  & \ddots & -b \\
        -c      & \cdots  & -c     & \lambda 
      \end{bmatrix}.
    \]
    and $q_*(t)=(\lambda - t)^n$, then by the previous result,
    \begin{align*}
      p_A(\lambda) &= \frac{-bq_*(-c) - (-c)q_*(-b)}{-c-(-b)} 
                    = \frac{b(\lambda+c)^n - c(\lambda+b)^n}{b-c}, 
                      & \text{if } b\ne c, \\ 
      p_A(\lambda) &= q_*(-b)-(-b)q_*\hp(-b) 
                    = (\lambda+b)^{n-1}(\lambda-(n-1)b),
                      & \text{if } b=c. 
    \end{align*}
  \end{proof}

  \paragraph{18.}
  \begin{proof}
    The identity can be derived immediately from Observation 1.2.4 and the 
    identity $a_1 = (-1)^{n-1}\tr\adj(A)$, the proof of which can be found on 
    p53.
  \end{proof}

  \paragraph{20.}
  \begin{proof}
    By (1.2.13), 
    \[
      \det(I+A) = (-1)^n p_A(-1) = 
      (-1)^n\left((-1)^n + \sum_{k=1}^n(-1)^{n-k}E_k(A)(-1)^k \right)
      = 1 + \sum_{k=1}^n E_k(A).
    \]
  \end{proof}

  \paragraph{22.} 
  \begin{proof}
    Suppose
    \[
      A = 
      \begin{bmatrix}
        t    & -1     &        & 0 \\
             & \ddots & \ddots &   \\
             &        & \ddots & -1 \\
             &        &        & t
      \end{bmatrix},
    \]
    then
    \begin{align*}
      p_{C_n(\vep)}(t)
      &= \det(A + [0,\dots,0,1]^T [-\vep,0,\dots,0]) \\
      &= \det A - \vep [1, 0,\dots,0](\adj A) [0,\dots,0,1]^T \\
      &= \det A - \vep ((\adj A)[1,n]) \\
      &= \det A - \vep \det A[\{n\}^c, \{1\}^c] \\
      &= t^n - \vep.
    \end{align*}
    And its spectrum, namely the set of roots of $p_{C_n(\vep)}$, is $\{
    \vep^{1/n} e^{2\pi ik/n}\,:\,k=0,1,\dots,n-1\}$. Hence,
    \[
      \rho(I+C_n(\vep)) = 1 + \rho(C_n(\vep)) = 1+\vep^{1/n}.
    \]
  \end{proof}  

% end

\subsection{Similarity}
  \paragraph{1.}
  \begin{proof}
    (a) Since $A$ and $B$ are diagonalizable and commute, by Theorem 1.3.21, 
    they are simultaneously diagonalizable. Hence, there exists some nonsingular
    $S\in M_n$ such that
    \begin{align*}
      A+B &=
      S\inv\diag(\lambda_1,\dots,\lambda_n)S 
      + S\inv\diag(\mu_{i_1},\dots,\mu_{i+n }) S \\
      &= S\inv\diag(\lambda_1+\mu_{i_1},\dots, \lambda_n+\mu_{i_n})S.
    \end{align*}
    Therefore, $\sigma(A+B)=\{\lambda_1+\mu_{i_1},\dots, \lambda_n+\mu_{i_n}\}$.
    \\ (b) By Exercise 1.1.6, $\sigma(B)=\{0\}$, completing the proof. \\
    (c) $\sigma(AB) = \{\lambda_1\mu_{i_1},\dots,\lambda_n\mu_{i_n}\}$, because
    \[
      S\inv(AB)S = (S\inv AS)(S\inv BS) = 
      \diag(\lambda_1\mu_{i_1},\dots,\lambda_n\mu_{i_n}).
    \]
  \end{proof}

  \paragraph{3.}
  \begin{proof}
    \[
      \sum_{k=0}^n a_k A^k = \sum){k=0}^n S\inv (a_k\Lambda^k)S
      = S\inv p(\Lambda)S.
    \]
  \end{proof}

  \paragraph{5.}
  \begin{proof}
    Let $A=\begin{bsmallmatrix} 0 & 1 \\ 0 & 0\end{bsmallmatrix}$ and $B=I$.
    Clear that $A$ and $B$ commute but are not simultaneously diagonalizable 
    since $A$ can not be diagonalized. This does not violate 1.3.12 because
    in 1.3.12, $A$ and $B$ are required to be diagonalizable.
  \end{proof}

  \paragraph{7.}
  \begin{proof}
    Suppose $B=\diag(b_1,\dots,b_n)$, then clear that $A=\diag(\sqrt{b_1},\dots,
    \sqrt{b_n})$ is a square root of $B$.\par
    Assume that such $A$ exists, then by Theorem 1.1.6, $\sigma(A)=\{0\}$, 
    implying that 
    \[
      A = \begin{bmatrix}
        0 & a \\ b & 0        
      \end{bmatrix}.
    \]
    Therefore,
    \[
      A^2 = \begin{bmatrix}
        ab & 0 \\ 0 & ab
      \end{bmatrix} \ne B.
    \]
    Contradiction.
  \end{proof}

  \paragraph{9.}
  \begin{proof}
    By Theorem 1.3.22, $AB$ and $BA$ have the same eigenvalues. And as similar
    matrices are of the same rank, $AB$ and $BA$ are not similar.
  \end{proof}

  \paragraph{11.}
  \begin{proof}
    Suppose that $A,B\in M_n$ commute and $Ax=\lambda x$ where $x\ne 0$ and $k$
    the is the smallest integer such that $B^kx \in \text{span}\{x,Bx,\dots,
    B^{k-1}x\} = \mathcal{S}$. For every $u=\sum_{i=0}^{k-1} x_iB^ix \in
    \mathcal{S}$, $Bu$ is a linear combination of $\sum_{i=0}^{k-2} x_iB^{i+1}x$
    and $x_{k-1}B^kx \in\mathcal{S}$. Hence $Bu\in\mathcal{S}$ and therefore 
    $\mathcal{S}$ is $B$-invariant. By Observation 1.3.18, there exists some 
    $0\ne y\in\mathcal{S}$ which is an eigenvector of $B$. Meanwhile, since
    \[
      A\sum_{i=0}^{k-1} x_iB^i x = \sum_{i=0}^{k-1} x_i(AB^i)x =
      \sum_{i=0}^{k-1} x_i B^i\lambda x = \lambda\sum_{i=0}^{k-1} x_iB^i x,
    \]
    every nonzero vector in $\mathcal{S}$ is a eigenvector of $A$ and so does 
    $y$. Hence, $A$ and $B$ have a common eigenvector $y$.\par
    Now we argue by induction on $m$, the size of the finite commuting family 
    $\mathcal{F}=\{A_1,\dots,A_m\}$. Suppose that $y\ne 0$ is a common 
    eigenvector of $A_1,\dots,A_{m-1}$ and let $k$ be the smallest integer such
    that $A^k_my\in \text{span}\{y,A_my,\dots, A_m^{k-1}y\}=\mathcal{S}$. Then 
    by some argument similar to the previous one, $\mathcal{S}$ is 
    $A_m$-invariant and hence contains a eigenvector $z$ of $A_m$. Meanwhile, 
    since $A_i$ and $A_m$ commute, every nonzero vector in $\mathcal{S}$ is an
    eigenvector of $A_i$ for $i=1,\dots,m-1$ and so does $z$, concluding that 
    matrices in a finite commuting families share a common eigenvector.\par
    $M_n$ is linear space of dimension $n^2$ and $\mathcal{F}$ is a subspace of 
    $M_n$ since for any $A,B,C\in\mathcal{F}$ and $a,b\in\mathbb{C}$,
    \[
      (aA+bB)C = a(AC) + b(BC) = a(CA) + b(CB) = C(aA+bB).
    \]
    Let $\mathcal{B}=\{B_1,\dots,B_k\}$ be a basis of $\mathcal{F}$. Since 
    $\mathcal{B}$ is finite and commuting, (b) shows that the matrices in 
    $\mathcal{B}$ have a common eigenvector $x$. Hence, supposing $B_ix=
    \lambda_i x$,
    \[
      \left(\sum_{i=1}^k b_iB_i\right)x = \sum_{i=1}^k b_i(B_ix) = 
      \left(\sum_{i=1}^k b_i\lambda_i\right)x
    \]
    where $b_i$ are some scalars. Thus, $x$ is a eigenvector of every $A\in
    \mathcal{F}$.
  \end{proof}

  \paragraph{13.}
  \begin{proof}
    Since similar diagonalizable matrices have the same eigenvalues and 
    multiplicities, their characteristic polynomials are therefore the same and
    vice versa.\par
    However, this is not true for two matrices which are not both 
    diagonalizable. For example, $\begin{bsmallmatrix} 0 & 1 \\ 0 & 0 
    \end{bsmallmatrix}$ and $0_{2\times 2}$ have the same characteristic 
    polynomial but they are not similar.
  \end{proof}

  \paragraph{15.}
  \begin{proof}
    Suppose that $A=S\Lambda S\inv$ where $\Lambda$ is a diagonal matrix, then
    \[
      p(A) = \sum_{k=0}^n a_k(S\Lambda S\inv)^k = 
      \sum_{k=0}^n a_kS\Lambda^kS\inv = Sp(\Lambda)S\inv.
    \]
    Clear that $p(\Lambda)$ is again a diagonal matrix. Hence, $p(A)$ is also
    diagonalizable. \par
    However the converse is not true. For example, $A = \begin{bsmallmatrix} 
    0 & 1 \\ 0 & 0 \end{bsmallmatrix}$ is not diagonalizable but $A^2 = 0$ 
    itself is a diagonal matrix.
  \end{proof}

  \paragraph{17.}
  \begin{proof}
    Suppose that $A=TBT\inv$ where $T\in M_n(\mathbb{R})$ is nonsingular, then
    $\bar{A}=\overline{TBT\inv} = \bar{T}\bar{B}\bar{T}\inv = T\bar{B}T\inv$ 
    since $T$ is real. And the converse is obviously true.
  \end{proof}

  \paragraph{19.}
  \begin{proof}
    Clear that $Q=Q^T$ and $Q^2 = I$, implying $Q=Q\inv$.\\
    (a) Suppose that $A=\begin{bsmallmatrix} A_{11} & A_{12} \\ A_{21} & A_{22}
    \end{bsmallmatrix}$. 
    \[
      0=K_{2n}A - AK_{2n} = 
      \begin{bmatrix}
        A_{21} - A_{12} & A_{22}-A_{11} \\ A_{11}-A_{22} & A_{12} - A_{21}
      \end{bmatrix}.
    \]
    Hence, $A$ is $2$-by-$2$ block centrosymmetric. And the proof of the 
    converse is trivial. If $A$ is nonsingular, then we have $A\inv K_{2n}=
    K_{2n}A\inv$, which implies $A\inv$ is $2$-by-$2$ block centrosymmetric. 
    Meanwhile, since $K_{2n}\inv = K_{2n}$, $K_{2n}AK_{2n} = A$. Suppose $B$ is 
    a $2$-by-$2$ block centrosymmetric matrix, then
    \[
      K_{2n}AB = K_{2n}A(K_{2n}BK_{2n}) = (K_{2n}AK_{2n})B_{2n}K_{2n} 
      = ABK_{2n}.
    \]
    Therefore, $AB$ is a $2$-by-$2$ block centrosymmetric matrix as well.\\
    (b)
    \begin{align*}
      Q\inv AQ &= \frac{1}{2}
      \begin{bmatrix} I_n & I_n \\ I_n & -I_n \end{bmatrix}
      \begin{bmatrix} B & C \\ C & B \end{bmatrix}
      \begin{bmatrix} I_n & I_n \\ I_n & -I_n \end{bmatrix} \\
      &= \begin{bmatrix} B+C & 0 \\ 0 & B+C \end{bmatrix} 
        = (B+C) \oplus (B-C).
    \end{align*} 
    (c) 
    \[
      \det A = \det(Q\inv AQ) = \det\begin{bmatrix}
        B+C & 0 \\ 0 & B+C
      \end{bmatrix} = \det(B^2+CB - BC-C^2)
    \]
    and $\rank A =\rank(B+C)+\rank(B-C)$ follows immediately from $Q\inv AQ=
    (B+C)\oplus(B-C)$.\\
    (d) $Q\inv \begin{bsmallmatrix} 0 & C \\ C & 0 \end{bsmallmatrix} = C\oplus
    (-C)$. Since $p_{C\oplus(-C)}(t)=p_C(t)p_{-C}(t)$, the eigenvalues occur in 
    $\pm$ pairs.
  \end{proof}

  \paragraph{23.}
  \begin{proof}
    Suppose that there exists some $X\in M_{n,m}$ such that $C=BX$ and let 
    $S=\begin{bsmallmatrix} I_n & X \\ 0 & I_m \end{bsmallmatrix}$. Clear that
    $S$ is nonsingular and $S\inv=\begin{bsmallmatrix} I_n & -X \\ 0 & I_m 
    \end{bsmallmatrix}$. Since
    \[
      S\begin{bmatrix}B & BX \\ 0_n & 0_m\end{bmatrix}S\inv = 
      \begin{bmatrix} B & 0 \\ 0 & 0_m\end{bmatrix},
    \]
    $A$ is similar to $B\oplus 0_m$.\par
    Now we suppose that $A$ is similar to $B\oplus 0_m$. Since similar matrices
    have the same rank, $\rank[B\,C] = \rank B$.    
  \end{proof}

  \paragraph{25.}
  \begin{proof}
    TODO
  \end{proof}

  \paragraph{29.}
  \begin{proof}
    Since $\det A=\sum_\sigma(\text{sgn}\sigma \prod_{i=1}^n a_{i\sigma(i)})$, 
    where $\sigma$ is any permutation of $\{1,2,\dots, n\}$, the determinant of 
    a matrix whose entries are integers is an integer.\par
    Suppose that the $a_{ij}$ is changed from $-1$ to $1$ and denote the new 
    matrix by $\tilde{A}$. Let $x,y\in\mathbb{C}^n$ be two vectors such that 
    $x_i=1$ and $y_j=2$, then $\tilde{A} = A + xy^T$. By Cauchy's identity,
    \[
      \det\tilde{A} = \det A + y^T(\adj A)x = \det A + 2\det A[\{i\}^c,\{j\}^c].
    \]
    Hence, the parity of $\det A$ is unchanged.\par
    \footnote{I don't know what $J_n$ actually is here and assume it to be the 
    matrix whose entries are all $1$.} Since changing a $-1$ entry to $1$ does 
    not change the parity of the determinant, we can change all the entries to 
    $1$. Hence, the parity of $\det A$ is the same as the parity of $\det(J_n-
    I)$. Induction on $n$ yields that it is opposite to the parity of $n$. Thus,
    if $n$ is even, then $\det A$ is odd and therefore nonzero, implying $A$ is 
    nonsingular.
  \end{proof}

  \paragraph{31.}
  \begin{proof}
    The characteristic polynomial of the matrix is $p(t)=(t-a)^2+b^2=
    (t-a-ib)(t-a+ib)$. Hence its eigenvalues are $a\pm ib$.
  \end{proof}

  \paragraph{33.}
  \begin{proof}
    $\,$\\
    (a) Since $A$ is real, $A\bar{x} = \overline{\bar{A}x} = \overline{Ax} = 
    \overline{\lambda x} = \bar{\lambda}\bar{x}$.\\
    (b) Since $\lambda$ is not real, $x$ and $\bar{x}$ are associated with
    different eigenvalues. Hence, $x$ and $\bar{x}$ are linear independent.
    Suppose that $mu+nv=0$. Then
    \[
      0= m(x+\bar{x})-in(x-\bar{x}) = (m-in)x + (m+in)\bar{x}.
    \]
    Since $m-in=0$ and $m+in=0$, $m=n=0$. Thus, $u$ and $v$ are also linear 
    independent.\\
    (c) 
    \[
      Au = \frac{1}{2}A(x+\bar{x})=\frac{1}{2}(\lambda x+\bar{\lambda}\bar{x})
      = \frac{1}{2}[(a+ib)(u+iv)+(a-bi)(u-iv)] = au-bv.
    \]
    Similarly, $Av=bu+av$. Hence,
    \[
      A[u, v] = [Au, Av] = [au-bv, bu+av] = [u, v]B.
    \]\\
    (d) Since $S\begin{bsmallmatrix} I_2 \\ 0 \end{bsmallmatrix} = [u\,v]$, 
    $S\inv[u\,v]=\begin{bsmallmatrix} I_2 \\ 0 \end{bsmallmatrix}$ and the proof
    of the next result is trivial. \\
    (e) Since $p_A(t) = p_B(t)p_{A_1}(t)$, the result amounts to the fact that
    $\lambda$ and $\bar{\lambda}$ are two roots of $p_B(t)$.
  \end{proof}
  

% end

