\section{Convex Sets}

\subsection{Definition of convexity}
  \paragraph{1.}
  \begin{proof}
    For $k=2$, $\theta_1x_1+\theta_2x_2\in C$ holds by by definition. We argue
    by induction on $k$ and assume that the inclusion holds for $k<m$. When $k=
    m$, denoting $\sum_{i=1}^{m-1}\theta_i$ by $s$,
    \[
      \sum_{i=1}^m \theta_ix_i = 
      s\sum_{i=1}^{m-1}\frac{\theta_ix_i}{s} + \theta_mx_m.
    \]
    Since $\sum_{i=1}^{m-1}\theta_i/s=1$, by the induction hypothesis,
    $\sum_{i=1}^{m-1}\theta_ix_i/s\in C$ . Meanwhile, as $s+\theta_m=1$, 
    $\sum_{i=1}^m\theta_ix_i\in C$, completing the proof.
  \end{proof}

  \paragraph{2.}
  \begin{proof}
    Clear that the intersection of two convex sets is still convex. Hence, the 
    intersection of $C\subset\mathbb{R}^n$ and any line is convex as long as $C$
    is convex.\par
    Now we suppose that the intersection of $C$ and any line is convex. For any 
    $x_1,x_2\in C$, $C_l=C\cap\{\theta x_1+(1-\theta)x_2:\,\theta\in\mathbb{R}
    \}$ is convex and therefore $\theta x_1+(1-\theta)x_2\in C_l\subset C$ for
    every $0\le\theta\le 1$. Thus, $C$ is convex.\par
    The above argument, \textit{mutatis mutandis}, gives the second result.
  \end{proof}

  \paragraph{3.}
  \begin{proof}
    For every $\theta\in[0,1]$, the process of bisecting the interval implies 
    there exists a series $\langle\delta_n\rangle$ whose sum is $\theta$. Hence,
    for every $a,b\in C$, $x_n = a+(b-a)\sum_{n=1}^\infty\delta_n$ converges to
    $a+\theta(b-a)$. Meanwhile, the midpoint convexity implies $x_n\in C$ for 
    every $n$. And since $C$ is closed, $a+\theta(b-a)\in C$. Thus, $C$ is 
    convex.
  \end{proof}

  \paragraph{4.}
  \begin{proof}
    Let $D$ be the intersection of all convex sets containing $C$. If $x\in C$,
    then its is a convex combination of some points in $C$. Hence, for every 
    convex set containing $C$, it contains $x$. Therefore, $\conv C\subset D$.
    For the converse, since $\conv C$ itself is a convex set containing $C$, $D
    \subset\conv C$. Thus, $\conv C=D$.
  \end{proof}

% end

\subsection{Examples}
  \paragraph{5.}
  \begin{solution}
    $|b_2-b_1|/\|a\|_2$.
  \end{solution}

  \paragraph{7.}
  \begin{proof}
    $\|x-a\|_2\le\|x-b\|_2$ iff $\langle x-a, x-a\rangle \le \langle x-b, x-b
    \rangle$ iff $2\langle x,b-a\rangle \le \langle b,b\rangle-\langle a,a
    \rangle$. Namely, $2(b-a)^Tx \le \|b\|_2^2-\|a\|_2^2$.
  \end{proof}

  \paragraph{2.8}
  \begin{proof}
    $\,$\par
    (a) It is trivial when $a_1$ and $a_2$ are linearly dependent, so we assume 
    that $a_1$ and $a_2$ are linearly independent. We first tackle the problem
    for orthonormal $a_1$ and $a_2$ and then reduce the general situation to it.
    \par
    Suppose that $a_1$ and $a_2$ are orthonormal. Let $S_0=\spn(a_1,a_2)$ and 
    $(b_1,\dots,b_{n-2})$ a basis of $S_0^\perp$. Then
    \[
      x\in S_0 \quad\Leftrightarrow\quad
      \begin{bmatrix}
        b_1^T \\ \vdots \\ b_{n-2}^T 
      \end{bmatrix}x = Bx = 0.
    \]
    For $y=y_1a_1+y_2a_2\in S_0$, $y_1\le 1$ iff $a_1^Ty \le 1$ as $(a_1,a_2)$
    is an orthonormal basis of $S_0$. Hence, 
    \[
      -1\le y_1,y_2\le 1 \quad\Leftrightarrow\quad
      \begin{bmatrix}
        a_1^T \\ a_2^T \\ -a_1^T \\ -a_2^T
      \end{bmatrix}y = Ay \preceq \mathbf{1}.
    \]
    Thus, for orthonormal $a_1$ and $a_2$, $S=\{x:\,Bx=0,\,Ax\preceq\mathbf{1}\}
    $, a polyhedron.\par
    Now we only assume the liner independence of $a_1$ and $a_2$. We know that
    there exists some invertible $n$-by-$n$ matrix\footnote{We can use $QR$ 
    factorization to construct the matrix explicitly} $R$ such that $[
    \tilde{a}_1,\tilde{a_2}]=R[a_1,a_2]$ and $\tilde{a}_1$ and $\tilde{a}_2$ are
    orthonormal. Denoting the set described in the problem with respect to $u_1$
    and $u_2$ by $S(u_1,u_2)$, $x\in S(a_1,a_2)$ iff $Rx\in S(\tilde{a}_1,
    \tilde{a_2})$ iff $Rx \in \{x:\,\tilde{B}x=0,\, \tilde{A}x\preceq 1\}$ where
    the meaning of $\tilde{A}$ and $\tilde{B}$ are described in the previous
    passage. Hence, 
    \[
      S(a_1,a_2) = \{x:\, \tilde{B}Rx=0,\, \tilde{A}Rx\preceq 1\}.
    \]
    
    (b) Yes, and the provided form has already satisfied the requirement.\par
    (c) No. Note that $\langle x,y\rangle_2\le1$ for all $y$ with $2$-norm $1$ 
    implies
    \[
      \|x\|_2 = \langle x,x/\|x\|\rangle_2 \le 1.
    \]
    And by the Cauchy-Schwarz inequality, for every $\|x\|\le 1$, $\langle x,y
    \rangle_2$ holds for every $\|y\|_2=1$. Hence, $S$ is the intersection of
    the unit ball and $\{x:\,x\succeq 0\}$, which is not a polyhedron.\par
    (d) Yes. Let $\tilde{S}=\{x\in\mathbb{R}^n:\,x\succeq 0,\,\|x\|_\infty\le 1
    \}$, which is clearly a polyhedron since when $x\succeq 0$, $\|x\|_\infty\le
    1$ is equivalent to $[e_1,\dots,e_n]x\preceq \mathbf{1}$ where $e_i$ is the 
    $i$-th vector in the standard basis of $\mathbb{R}^n$.\par
    Now we show that $S=\tilde{S}$. Suppose that $x\succeq 0$. If $\langle x,y
    \rangle_2\le 1$ for all $y$ with $1$-norm $1$, then $x_i = \langle x,e_i
    \rangle_2 \le 1$. Namely, $\|x\|_\infty\le 1$. Meanwhile, if $\|x\|_\infty
    \le 1$, 
    \[
      \langle x,y\rangle \le \sum_{i=1}^n x_i|y_i| \le 1
    \]
    as it is just the weighted average of $x_1,\dots,x_n$. Hence, $S=\tilde{S}$,
    completing the proof.
  \end{proof}

  \paragraph{2.9}
  \begin{proof}
    $\,$\par
    (a) By the definition, 
    \begin{align*}
      x\in V &\Leftrightarrow \|x-x_0\|_2^2-\|x-x_i\|_2^2 \le 0  \\
      &\Leftrightarrow 2\langle x,x_i-x_0\rangle 
        \le \langle x_i,x_i\rangle - \langle x_0,x_0\rangle
        \quad\text{for $i=1,\dots,K$}\\
      &\Leftrightarrow
        2\begin{bmatrix}
          \langle x,x_1-x_0 \rangle \\ \vdots \\ \langle x,x_K-x_0 \rangle
        \end{bmatrix}
        \preceq\begin{bmatrix}
          \|x_1\|_2^2-\|x_0\|_2^2 \\ \vdots \\ \|x_K\|_2^2-\|x_0\|_2^2
        \end{bmatrix} \\
      &\Leftrightarrow
      2\begin{bmatrix}
        (x_1-x_0)^T \\ \vdots \\ (x_K-x_0)^T
      \end{bmatrix}x
      \preceq\begin{bmatrix}
        \|x_1\|_2^2-\|x_0\|_2^2 \\ \vdots \\ \|x_K\|_2^2-\|x_0\|_2^2
      \end{bmatrix}
    \end{align*}
    Hence, $V$ is a polyhedron. Intuitively, the border of a Voronoi set are the 
    lines with the same distances to $x_0$ and $x_i$. \par
    (b) Suppose that $P=\{x:\,\alpha_k^Tx\le b_k, k=1,\dots,K\}$. Let $x_0$ be
    any point of $P$ and we construct the other points by reflection. For each 
    $k$, let $\tilde{x}_k$ be any point of $\{x:\,\alpha_k^Tx=b_k\}$, $U_k=I-2
    \alpha_k\alpha_k^T/\|\alpha_k\|^2_2$, the Householder matrix, and 
    \[
      R_k(x) = U_k(x-\tilde{x}_k)+\tilde{x}_k = 
      x + 2\frac{\alpha_k}{\|\alpha_k\|_2^2}(b_k-\alpha_k^Tx).
    \]
    It is easy to verified that $P$ is the Voronoi region of $x_0$ with respect
    to $R_1(x_0),\dots,R_K(x_0)$.\par
  \end{proof}

  \paragraph{10.}
  \begin{proof}
    $\,$\par
    (a) Suppose $x_1,x_2\in C$ and $\theta\in(0,1)$. Let $x=\theta x_1+(1-
    \theta)x_2$. Since $A$ is symmetric, $x_2^TAx_1 = x_1^TAx_2$. Thus,
    \begin{align*}
      f(x)
      &= x^TAx+b^Tx+c \\
      &=\theta^2 x_1^TAx_1+2\theta(1-\theta)x_1^TAx_2 + (1-\theta)^2x_2^TAx_2\\
      &\quad+\theta b^Tx_1 + (1-\theta)b^Tx_2 + \theta c + (1-\theta)c.
    \end{align*}
    Note that
    \begin{align*}
      \theta^2x_1^TAx_1+\theta b_1^Tx_1 + \theta c 
      &= \theta(x_1^TAx_1 + b_1^Tx_1+c) - \theta(1-\theta)x_1^TAx_1\\
      &\le -\theta(1-\theta)x_1^TAx_1
    \end{align*}
    and we can get a similar inequality for $x_2$. Hence,
    \begin{align*}
      f(x) &\le -\theta(1-\theta)(x_1^TAx_1 - 2x_1^TAx_2 + x_2^TAx_2) \\
      &= -\theta(1-\theta)(x_1-x_2)^TA(x_1-x_2) \le 0
    \end{align*}
    as $A\succeq 0$. Hence, $C$ is convex.\par
    (b) Put $H=\{x:\, g^Tx+h=0\}$, $B=A+\lambda gg^T$ and 
    \[
      C_B=\{x\in \mathbb{R}^n:\, x^TBx+b^Tx+c-\lambda h^2\le 0\}.
    \]
    By (a), $C_B$ is convex and so does $C_B\cap H$. Suppose $x\in H$, then 
    $x^TBx=x^TAx +\lambda h^2$. Therefore, $C_B\cap H=C$. Thus, $C$ is convex.
  \end{proof}
% end

\subsection{Operations that preserve convexity}
  \paragraph{16.}
  \begin{proof}
    For every $(a,b_1+b_2),(c,d_1+d_2)\in S$ and $0\le\theta\le 1$, let 
    \[
      z_\theta = \theta(a,b_1+b_2)+(1-\theta)(c,d_1+d_2) = (x,y_1+y_2)
    \]
    where
    \begin{align*}
      x = \theta a+(1-\theta) c,\quad
      y_i = \theta b_i+(1-\theta)d_i\quad\text{for $i=1,2$}.
    \end{align*}
    Since $S_i$ is convex and $(a,b_i),(c,d_i)\in S_i$,
    \[
      (x,y_i)=\theta(a,b_i) + (1-\theta)(c,d_i)\in S_i.
    \]
    Hence, $S$ is convex.
  \end{proof}

  \paragraph{18.}
  \begin{proof}
    Let $\theta:\mathbb{R}^n\to\mathbb{R}^{n+1}$ be defined by $x\mapsto (x,1)$
    and $P:\mathbb{R}^{n+1}\to\mathbb{R}^n$ the perspective function. It can be
    verified that $f=P\circ Q\circ\theta$. Now we show that $g=P\circ Q\inv
    \circ\theta$ is the inverse of $f$. Clear that $P\circ\theta=I$, the 
    identity map on $\mathbb{R}^n$. Hence,
    \[
      f\circ g=P\circ Q\circ\theta\circ P\circ Q\inv\circ\theta = I.
    \]
    Similarly, $g\circ f=I$. Thus, $f$ is invertible and $g=f\inv$.
  \end{proof}
% end
\subsection{Separation theorems and supporting hyperplanes}
  \paragraph{20.}
  \begin{proof}
    Let $N=\null A$ and $x_0$ be such that $Ax_0=b$. We prove the hint first.
    Suppose for all $x\in N$, $\langle x_0+x,c\rangle=d$. Hence, $\langle x_0,
    c\rangle+\langle x,c\rangle=d$, which implies $\langle x,c\rangle=0$ and
    \begin{equation}
      \label{eq:1.20-1}
      \langle x_0, c\rangle=d.
    \end{equation}
    Since $\langle x,c\rangle=0$ for all $x\in N$, $N=\nul A\subset\{c\}^\perp$
    and therefore, $\range A^T\supset\{c\}$. Thus, there exists a $\lambda$ such
    that $A^T\lambda=c$. Substituting this into \eqref{eq:1.20-1} yields
    \[
      d=\langle x_0,A^T\lambda\rangle=\langle Ax_0,\lambda\rangle=b^T\lambda.
    \]
    And the proof of the converse is straightforward.\par
    Now we show the proposition. First we suppose such an $x$ does not exist.
    Namely, $D=x_0+N$ and $\mathbb{R}^n_{++}$ are disjoint. Since $D$ is an 
    affine set and $\mathbb{R}^n_{++}$ is convex and open, by the converse 
    separating theorem, there exists some nonzero $c\in\mathbb{R}^n$ and scalar 
    $d$ such that $c^Ty\le d$ for all $y\in D$ and $c^Ty\ge d$ for all $y\in C$. 
    Since the image of an affine set under a linear mapping is still an affine 
    set, $c^Ty\le d$ for all $y\in D$ implies $c^Ty=d$ for all $y\in D$. Then, 
    by our previous result, there exists a $\lambda$ such that $c=A^T\lambda$ 
    and $d=b^T\lambda$. Since $c\ne 0$, $A^T\lambda\ne 0$. Meanwhile, from $c^T
    y\ge d$ for all $y\in C$ we conclude $y\succeq 0$, otherwise we may choose 
    $y\in C$ which is a large positive number on the position where the
    component of $y$ is negative and zero elsewhere to lead to a contradiction.
    Thus, $A^T\lambda\succeq 0$. Finally, with the same approach, we conclude
    that $d\le 0$ and therefore $b^T\lambda\le 0$.\par
    For the converse, our discussion shows that the existence of such a 
    $\lambda$ implies a separating hyperplane of $C$ and $D$. Since $C$ is open,
    it does not intersect with the separating hyperplane. Hence, there is no $x$ 
    satisfying $x\succ 0$ and $Ax=b$, completing the proof.
  \end{proof}
  
  \paragraph{22.} TODO
  
  \paragraph{23.}
  \begin{proof}
    $A=\{(x,y)\in\mathbb{R}^2:\,y\le 0\}$ and $B=\{(x,y)\in\mathbb{R}^2:\,x>0,
    y\ge 1/x\}$.
  \end{proof}
  
  \paragraph{25.}
  \begin{proof}
    Since $P_{\text{inner}}=\conv\{x_1,\dots,x_K\}$ is the smallest convex set
    that contains $\{x_1,\dots,x_K\}$, $\{x_1,\dots,x_k\}\subset C$ as $C$ is
    closed and $C$ is convex, $P_{\text{inner}}\subset C$. Meanwhile, it follows
    from the definition that $C\subset P_{\text{outer}}$.
  \end{proof}
  
  \paragraph{26.}
  \begin{proof}
    If $C=D$, then clear that $S_C=S_D$. For the converse, we argue by 
    contradiction. Assume the existence of some $x_0\in C$ such that $x_0\notin 
    D$. Since $D$ is closed and convex, there exists a hyperplane strictly 
    separate $x_0$ and $D$, that is, there exists some nonzero $a\in\mathbb{R}
    ^n$ and $b\in\mathbb{R}$ such that $a^Tx<b$ for all $x\in D$ and $a^Tx_0>b$.
    Then by the definition of the support function, 
    \[
      S_C(a)\ge a^Tx_0 > b > a^Tx,\quad\text{for all $x\in D.$}
    \]
    Hence, $S_C(a)>\sup_{x\in D}a^Tx=S_D(a)$. Contradiction. Thus, $C\subset D$.
    Interchanging the roles of $C$ and $D$ yields $C\supset D$. Therefore, 
    $C=D$.
  \end{proof}
  
  \paragraph{27.} TODO
% end
\subsection{Convex cones and generalized inequalities}
  \paragraph{31.}
  \begin{solution}
    $\,$\par
    (a) For $\lambda_1,\lambda_2\in K^*$ and $\theta_1,\theta_2>0$, since
    \[
      f=\langle\cdot,\theta_1\lambda_1+\theta_2\lambda_2\rangle=
      \theta_1\langle\cdot,\lambda_1\rangle+
      \theta_2\langle\cdot,\lambda_2\rangle,
    \]
    $f$ also maps $K$ into $\mathbb{R}_+$. Namely, $\theta_1\lambda_1+\theta_2
    \lambda_2\in K^*$.\par
    (b) If $f=\langle\cdot,\lambda\rangle$ maps $K_2$ into $\mathbb{R}_+$, then,
    as $K_1\subset K_2$, it maps $K_1$ into $\mathbb{R}_+$. Thus, $K_2^*\subset
    K_1^*$.\par
    (c) Suppose $(\lambda_n)\subset K^*$ be a sequence converging to $\lambda\in
    \mathbb{R}^n$. Then, by the continuity of the inner product, for every $x
    \in K$, $\langle x,\lambda\rangle=\lim_{n\to\infty}\langle x,\lambda_n
    \rangle\ge 0$. Hence, $\lambda\in K^*$. Namely, $K^*$ is closed.\par
    (d) If $y\in\inte K^*$, then there exists some $\vep>0$ such that for all 
    $\Delta y$ with $\|\Delta y\|<\vep$, $y+\Delta y\in K$, that is, $(y+\Delta
    y)^Tx\ge 0$ for all $x\in\cl K$. For each $x$, put $\Delta y=-\vep x/
    2\|x\|$ and then we obtain $y^Tx>0$.\par
    For the converse, suppose that $y\notin\inte K^*$. Namely, for all $\vep>0$,
    there exists some $\Delta y$ with $\|\Delta y\|<\vep$ such that $(y+\Delta
    y)^Tx_0\le 0$ for some $x_0\in\cl K$. This time, put $\Delta y=\vep x_0/2
    \|x_0\|$ and then we get $y^Tx_0\le 0$.\par
    (e) We argue by contradiction. Assume that there exists some nonzero $y\in 
    K^*$ such that $-y\in K^*$. Then for every $x\in K$, $\langle x,\pm y\rangle
    \ge 0$, which yields $\langle x,y\rangle=0$, i.e., $K\subset\{y\}^T$. Since
    $\dim\{y\}^T<n$, $K$ can not have nonempty interior. Contradiction. Thus, 
    $K^*$ is pointed.\par
    (f) For every $x\in\cl K$, $x^Ty\ge 0$ for all $y\in K^*$. Hence, $x\in K^{
    **}$. Thus, $\cl K\subset K^{**}$. For the converse, note that $\cl K$, a 
    closed convex cone, is fully determined by its supporting hyperplanes at the
    origin. Namely, if $x$ satisfies $y^Tx\ge 0$ for all $y\in(\cl K)^*=K^*$, 
    then $x\in\cl K$. From this we conclude $K^{**}\subset \cl K$. Thus, $K^{**}
    =K$.\par
    (g) We argue by contradiction. Assume that $\inte K^*$ is empty. Then, by 
    (d), if $y\in K^*$, then $y^Tx=0$ for all $x\in\cl K$. Namely, $K^*\subset
    (\cl K)^\perp$. Therefore, $(K^*)^\perp\supset\cl K=K^{**}$ where the 
    equality comes from (f). Thus, for all $x\in K^**$, $-x^Ty=x^Ty=0$ for all 
    $y\in K^*$, which contradict the assumption that $K$ is pointed. Thus, 
    $\inte K^*\ne\varnothing$. (This proof should be reviewed.)
  \end{solution}
  
  \paragraph{32.}
  \begin{solution}
    $\langle y,Ax\rangle\ge 0$ for all $x\succeq 0$ iff $\langle A^Ty,x\rangle
    \ge 0$ for all $x\succeq 0$ iff $A^Ty\succeq 0$. Hence, $K^*=\{y:\, A^Ty
    \succeq 0\}$.
  \end{solution}
  
  \paragraph{35.}
  \begin{proof}
    Denote this set by $\mathcal{C}$. Note that $z^TXz=\tr(zz^TX)$. Hence, $X$
    is copositive iff $\langle zz^T,X\rangle\ge 0$ for all $z\succeq 0$. Namely,
    \begin{equation}
      \label{eq:1.35}
      \mathcal{C}=
      \bigcap_{z\succeq 0}\{X\in\mathbf{S}^n:\, \langle zz^T,X\rangle\ge 0\},
    \end{equation}
    the intersection of some half spaces. Hence, $\mathcal{C}$ is a closed 
    convex cone. Since $\mathcal{C}$ contains the set of all positive 
    semidefinite matrices, it is solid. Meanwhile, if $\pm X\in\mathcal{C}$, 
    then $z^TXz=0$ for all $z\succeq 0$. Hence, $X=0$. Thus, $\mathcal{C}$ is a
    proper cone.\par
    Note that $\mathcal{C}^*$ is just the collection of the inward normal 
    vectors of supporting hyperplanes of $\mathcal{C}$ at the origin. By
    \eqref{eq:1.35}, $\mathcal{C}^*=\{zz^T:\, z\succeq 0\}$.
  \end{proof}
% end


























