%---------%---------%---------%---------%---------%---------%---------%---------
\section{Convex Functions}
\subsection{Definition of convexity}
  \paragraph{1.}
  \begin{proof}
    $\,$\par
    (a) Clear that $\frac{b-x}{b-a},\frac{x-a}{b-a}\ge 0$ and the sum of them 
    is $1$ for all $x\in[a,b]$. Hence, by the definition of convexity,
    \[
      f(x) = f\left(\frac{b-x}{b-a}a + \frac{x-a}{b-a}b\right) 
      \le \frac{b-x}{b-a}f(a) + \frac{x-a}{b-a}f(b).
    \]\par
    (b) By (a),
    \[
      \frac{f(x)-f(a)}{x-a} \le 
      \frac{\dfrac{b-x}{b-a}f(a)+\dfrac{x-a}{b-a}f(b)-f(a)}{x-a} = 
      \frac{f(b)-f(a)}{b-a}.
    \]
    And a similar argument gives the second inequality.\par
    (c) Just let $x$ approach $a$ and $b$ respectively and we get these two 
    inequalities.\par
    (d) By (c), for every $a<b\in\dom f$,
    \begin{equation*}
      f\hp(b)-f\hp(a) \ge \frac{f(b)-f(a)}{b-a}-f\hp(a) \ge 0.
    \end{equation*}
    Let $a\to b-$ and we get $f^{\prime\prime}(b-)\ge 0$. Since $f$ is twice
    differentiable, this implies $f^{\prime\prime}(b)\ge 0$. This argument,
    \textit{mutatis mutandis}, yields $f^{\prime\prime}(a)\ge 0$.
  \end{proof}

  \paragraph{3.}
    There is another proof which shows the concavity by showing the convexity of
    $\hypo g$. But I think there exists some faults related to the domain of $f$
    in that proof.
  \begin{proof}
    We show that $g$ is concave. For every $y_1,y_2\in (f(a), f(b))$, suppose
    $y_1=f(x_1)$ and $y_2=f(x_2)$. Since $f$ is convex, 
    \[
      \frac{y_1+y_2}{2} = \frac{f(x_1)+f(x_2)}{2} \ge 
      f\left(\frac{x_1+x_2}{2}\right).
    \]
    Since $f$ is increasing, so is $g$. Hence,
    \[
      g\left(\frac{y_1+y_2}{2}\right)\ge 
      g\left(f\left(\frac{x_1+x_2}{2}\right)\right) =
      \frac{x_1+x_2}{2} = 
      \frac{1}{2}g(y_1)+\frac{1}{2}g(y_2).
    \]
    Thus, $g$ is concave.
  \end{proof}
  
  \paragraph{5. Running average of a convex function}
  \begin{proof}
    Put $t=sx$, then
    \[
      F(x)=\frac{1}{x}\int_0^1f(sx)\rd(sx)=\int_0^1f(sx)\rd s.
    \]
    It can be verified that for fixed $s$, $f(sx)$ is convex in $x$. Hence, for
    every $\lambda\in(0,1)$, $a,b\in\dom F$,
    \[
      F(\lambda a+(1-\lambda)b)\le
      \int_0^1\left\{\lambda f(sa)+(1-\lambda)f(sb) \right\}=
      \lambda F(a)+(1-\lambda)F(b).
    \]
    Thus, $F$ is convex.
  \end{proof}
  
  \paragraph{8. Second-order condition for convexity}
  \begin{proof}
    First we prove the case $f:\mathbb{R}\to\mathbb{R}$. If $f$ is convex, then
    $\dom f$ is convex by definition. Meanwhile, for every $x$ and $t$, by the 
    first-order condition,
    \[
      \frac{f(x+t)-f(x)-f\hp(x)t}{t^2}\ge 0.
    \]
    Let $t\to 0$ and we obtain $f^{\prime\prime}(x)\ge 0$. For the converse, 
    $f^{\prime\prime}(x)$ implies that $f\hp$ is monotonically increasing. Thus,
    by the mean-value theorem, there exists some $c$ between $x$ and $y$ such 
    that
    \[
      f(y)-f(x)=f\hp(c)(y-x) \ge f\hp(x)(y-x),
    \]
    Namely, $f$ is convex.\par
    Now we prove the general case. Recall that $f$ is convex iff $f$ is convex
    along all lines. For fixed $x,u\in\mathbb{R}^n$, define $g(t)=f(x+tu)$. By
    our previous result, $g$ is convex iff 
    \begin{align*}
      0\le g^{\prime\prime}(t)=u^T\nabla^2f(x_0+tu)u\quad\text{for all $t$.}
    \end{align*}
    Namely, $\nabla^2f(x)\succeq 0$ for all $x\in\mathbb{R}^n$.
  \end{proof}
  
%  \paragraph{9. Second-order conditions for convexity on an affine set}
%  \begin{proof}
%  \end{proof}

  \paragraph{13.}
  \begin{proof}
    Define $f(x)=\sum_{i=1}^n x_i\log x_i$. Some computation yields $D_{kl}(u,v)
    =f(u)-f(v)-\nabla f(v)^T(u-v)$. The inequality and the equality condition
    follows immediately from the fact that $f$ is strictly convex.
  \end{proof}
% end

\subsection{Examples}
  \paragraph{16.}
  \begin{solution}
    $\,$\par
    (a) Convex. For every $x\in\mathbb{R}$, $f^{\prime\prime}(x)=e^x>0$.\par
    (b) Quasiconcave. For every $(x_1,x_2)^T\in\mathbb{R}_{++}^2$, $\nabla^2f=
    \begin{bsmallmatrix} 0 & 1 \\ 1 & 0\end{bsmallmatrix}$, which is neither
    positive semidefinite nor negative semidefinite. Hence, $f$ is not convex or
    concave. Its superlevel sets $S_\alpha$, however, are convex as
    \[
      \frac{(x_1+x_2)(y_1+y_2)}{4} \ge \sqrt{x_1x_2y_1y_2} \ge \alpha
    \]
    as long as $(x_1,x_2),(y_1,y_2)\in S_\alpha$.\par
    (c) Convex. For every $(x_1,x_2)\in \mathbb{R}_{++}^2$, 
    \[
      \nabla^2f(x_1,x_2) = \frac{1}{x_1x_2}
      \begin{bmatrix}
        \dfrac{2}{x_1^2} & \dfrac{1}{x_1x_2} \\
        \dfrac{1}{x_1x_2} & \dfrac{2}{x_2^2}
      \end{bmatrix}.
    \]
    Since both $2/x_1^3x_2$ and $\det(\nabla^2 f)$ are positive, $\nabla^2 f$ is
    positive definite. Thus, $f$ is convex.\par
    (d) Quasilinear. For every $(x_1, x_2)\in\mathbb{R}_{++}^2$,
    \[
      \nabla^2f(x_1,x_2) = \begin{bmatrix}
        0 & -1/x_2^2 \\ -1/x_2^2 & 2x_1/x_2^3
      \end{bmatrix},
    \]
    which is neither positive nor negative semidefinite since $(x\pm\sqrt{x_1^2+
    x_2^2})/x_2^3$, the eigenvalues of $\nabla^2f$, always have different signs.
    However, since it sublevel sets $S_\alpha=\{(x_1,x_2)\in\mathbb{R}_{++}^2:\,
    x_1/x_2\le\alpha\}=\{(x_1,x_2)\in\mathbb{R}_{++}^2:\,[1,-\alpha][x_1,x_2]^T
    \le 0\}$, which is convex, $f$ is quasiconvex. Similarly, $f$ is 
    quasiconcave. Thus, $f$ is quasilinear.\par
    (e) Convex. For every $(x_1,x_2)\in\mathbb{R}\times\mathbb{R}_{++}$,
    \[
      \nabla^2f(x_1,x_2) = \begin{bmatrix}
        1/x_2 & -2x_1/x_2^2 \\ -2x_1/x_2^2 & 2x_1^2/x_2^3
      \end{bmatrix},
    \]
    which is positive semidefinite since both $1/x_2$ and $\det(\nabla^2 f)$ are
    nonnegative. \par
    (f) Concave. For every $(x_1,x_2)\in\mathbb{R}_{++}^2$,
    \[
      \nabla^2f(x_1,x_2) = \alpha(\alpha-1)x_1^\alpha x_2^{1-\alpha}
      \begin{bmatrix}
        x_1^{-2} & -(x_1x_2)\inv \\ -(x_1x_2)\inv & x_2^{-2}
      \end{bmatrix},
    \]
    which is negative definite since both $\alpha(\alpha-1)x_1^\alpha
    x_2^{1-\alpha}x_1^{-2}$ and $\det(\nabla^2 f)$ are negative.
  \end{solution}
  
  \paragraph{17.}
  \begin{proof}
    Put $z_k=(x_1^k,\dots,x_n^k)$. Then the Hessian of $f$ is
    \[
      \nabla^2f(x)=
      (1-p)(\mathbf{1}^Tz_p)^{1/p-2}
      (z_{p-1}z_{p-1}^T-\mathbf{1}^Tz_p\diag(z_{p-2})).
    \]
    Put $K=(1-p)(\mathbf{1}^Tz_p)^{1/p-2}$, a nonnegative constant. For every 
    $v\in\mathbb{R}^n$,
    \begin{align*}
      v^T\nabla^2f(x)v
      &=Kv^T(z_{p-1}z_{p-1}^T-\mathbf{1}^Tz_p\diag(z_{p-2}))v \\
      &=K\left\{\left(\sum_{i=1}^nv_ix_i^{p-1}\right)^2-
      \left(\sum_{i=1}^nx_i^p\right)
      \left(\sum_{i=1}^nx_i^{p-2}v_i^2\right)\right\}\\
      &\le 0,
    \end{align*}
    where the inequality comes from the Cauchy-Schwarz inequality $(a^Tb)^2\le
    (a^Ta)(b^Tb)$ with $a_i=x_i^{p/2}$ and $b_i=x_i^{p/2-1}v_i$. Thus, $f$ is 
    concave.
  \end{proof}
  
  \paragraph{19. Nonnegative weighted sums and integrals}
  \begin{proof}
    $\,$\par
    (a) For each $k=1,\dots,r$, let $f_k(x)=\sum_{i=1}^k x_[i]$, which is
    convex. Put
    \[
      \beta_1=\alpha_1-\alpha_2,\quad
      \beta_2=\alpha_2-\alpha_3,\quad
      \dots\quad
      \beta_r=\alpha_r.
    \]
    Since $\alpha_1\ge\alpha_2\ge\cdots\ge\alpha_r$, $\beta_i\ge 0$ for $i=1,
    \dots,r$. Hence, $f=\beta_1f_1+\cdots+\beta_rf_r$, being a nonnegative
    weighted sum of convex functions, is convex.\par
    (b) Note that $T(x,\omega)$ is linear in $x$ for fixed $\omega$. Hence, it 
    can be verified via definition the convexity of $\dom f$ and $-\log T(x,
    \omega)$ is also convex in $x$. Hence, $f(x)=\int_{0}^{2\pi}\{-\log T(x,
    \omega)\}\rd\omega$ is convex.
  \end{proof}
  
  \paragraph{21.}
  \begin{proof}
    $\,$\par
    (a) By Prob.20(a), $\|A^{(i)}x-b^{(i)}\|$ is convex for each $i=1,\dots,k$
    and consequently $f$, the pointwise maximum of them, is convex.\par
    (b) Let $E\subset\mathbb{R}^n$ be the collection of all vectors whose 
    entries are $\pm 1$ or $0$. Then for each $c\in E$, $x\mapsto c^Tx$ defines 
    a convex function. Since $f(x) = \max_{c\in E}c^Tx$, it is also convex.
  \end{proof}
  
  \paragraph{22.(a)}
  \begin{proof}
    Put $g(y)=\log(\sum_{i=1}^ne^{y_i})$ and $h(x)=Ax+b$ where $A=[a_1,\dots,
    a_n]^T$ and $b=[b_1,\dots,b_n]^T$. Then $j=g\circ h$ is convex on 
    $\mathbb{R}^n$. Hence, $\dom f=\{x:\, j(x)<1\}$ is convex. Meanwhile, $-j$
    is concave, $-\log$ is convex and the extension of it to $\mathbb{R}$ is
    non-increasing. Therefore, $f(x)=-\log(-j(x))$ is convex.
  \end{proof}
% end




















